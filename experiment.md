# **Interesting Experiences**

If you come across more interesting findings, feel free to add them here and make a pull request. We’d love to hear them.

### **Insight into Chats**
Admins of Open WebUI can see the chats of other users. Unless these were started as temporary chats by clicking on the model name in the top left and then selecting “temporary chat.”

### **Uncensored Models**
You may be familiar with the restrictions of commercially offered chat services. A model like `llama2-uncensored`, however, has no problem giving instructions for illegal activities. Topics that are politically sensitive may also produce answers different from what you’d expect.

### **Cultural Context**
Almost all models can communicate in Dutch as well, though not all equally well. Also, not all models are equally familiar with all cultures—try, for example, asking for a well-known Dutch proverb or about *Bassie en Adriaan*.

### **(Unexpected) Limitations**
Ask a model how many times the letter 'r' appears in the word *strawberry*. Sometimes they get it right, but if you then say they were wrong, they may still give an incorrect answer. It can be interesting to gain experience with these sometimes unexpected limitations.

### **Make a Model Behave Differently**
With the help of a [system prompt](https://docs.openwebui.com/features/chat-features/chat-params/#system-prompt-and-advanced-parameters-hierarchy-chart), you can instruct a model to answer all questions as if it were Julius Caesar (or a Gaul from a small village). Or maybe to answer everything in French. Or perhaps to assist with math problems but never give the direct answer. This is a low-threshold way to realize all kinds of applications. Creating a really good system prompt that does exactly what you want may require some experimentation.
